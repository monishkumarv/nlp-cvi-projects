{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference link for using Word Embeddings from genesim library - https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Out-of-Vocabulary Words in Natural Language Processing based on Context - Better way than the one I followed below - [Link](https://medium.com/@shabeelkandi/handling-out-of-vocabulary-words-in-natural-language-processing-based-on-context-4bbba16214d5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Google's Word2Vec: (300 dimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors \n",
    "filename = r'C:\\Users\\Monish Kumar\\Python projects\\# Natural Language Proccessing\\GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training custom Word2Vec on imdb dataset: 300 dimensional\n",
    "#### Attributes of genism Word2Vec:\n",
    "- size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "- window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "- min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "- workers: (default 3) The number of threads to use while training.\n",
    "- sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\n",
    "- negative = 10 - for negative sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Loading the imdb dataset\n",
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded imdb dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))\n",
    "\n",
    "# Converting the words from numbers to words\n",
    "word_index = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word_index.items()}\n",
    "imdb_dataset = pd.Series(X_train).apply(lambda x: [id2word.get(i) for i in x ]) \n",
    "\n",
    "# Train our custom model on the imdb dataset\n",
    "embedding_size=300\n",
    "word2vec_custom = Word2Vec(imdb_dataset, min_count = 1, size = embedding_size, window = 5)\n",
    "\n",
    "# Saving the model\n",
    "word2vec_custom.save('word2vec_custom_imdb.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This word (diane keaton) is present in imdb dataset but not in Googles's Word2Vec\n",
    "word2vec_imdb['keaton']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "def train_lstm(X_train, y_train, embedding_matrix, embedding_trainable, embedding_size=300, batch_size = 64, num_epochs = 3):\n",
    "    \n",
    "    embedding_size=300 \n",
    "\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocabulary_size + 1, embedding_size, \n",
    "                        weights=[embedding_matrix], input_length=max_words, \n",
    "                        trainable = embedding_trainable))\n",
    "    model.add(LSTM(100))\n",
    "    # model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "\n",
    "    batch_size = 64\n",
    "    num_epochs = 3\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train2, X_valid, y_train2, y_valid = train_test_split(X_train,y_train, test_size = 0.33, random_state = 123)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and padding the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Review': ['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'musicians', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'landed', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'frog', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'barrel', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'heartfelt', 'film', 'want', 'an']\n",
      "\n",
      "'Label': 1\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word_index.items()}\n",
    "print(\"'Review':\",[id2word.get(i, ' ') for i in X_train[0]])\n",
    "print(\"\\n'Label':\",y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n",
      "Minimum review length: 70\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(len(max((X_train + X_test), key=len))))\n",
    "print('Minimum review length: {}'.format(len(min((X_train + X_test), key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_words = 1000\n",
    "\n",
    "# By default Pre-Sequence Truncation is followed - if you want to use Post-Sequence Truncation use truncating='post'\n",
    "# By default Pre-Sequence Padding is followed - if you want to use Post Padding use padding='post'\n",
    "X_train = pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Not removing the words not available in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of words not available in the dictionary: 714\n"
     ]
    }
   ],
   "source": [
    "# Using a vector of zeros for every missing word\n",
    "embedding_dim = len(word2vec['hi']) # 300!\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "embedding_matrix = np.zeros((vocabulary_size + 1, embedding_dim))\n",
    "id2word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "count = 0\n",
    "for i in range(1,vocabulary_size + 1):\n",
    "    word = id2word.get(i)\n",
    "    try: \n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "    except:  # if word not present in the dictionary - leave the row of the embedding matrix to be empty\n",
    "        count +=1\n",
    "        \n",
    "print('No. of words not available in the dictionary:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1000, 300)         3000300   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,160,801\n",
      "Trainable params: 3,160,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Monish Kumar\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16750 samples, validate on 8250 samples\n",
      "Epoch 1/3\n",
      "16750/16750 [==============================] - 1773s 106ms/step - loss: 0.5363 - accuracy: 0.7230 - val_loss: 0.4059 - val_accuracy: 0.8286\n",
      "Epoch 2/3\n",
      "16750/16750 [==============================] - 1848s 110ms/step - loss: 0.3218 - accuracy: 0.8695 - val_loss: 0.3480 - val_accuracy: 0.8702\n",
      "Epoch 3/3\n",
      "16750/16750 [==============================] - 29192s 2s/step - loss: 0.2204 - accuracy: 0.9174 - val_loss: 0.3197 - val_accuracy: 0.8701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x235fe59db08>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lstm(X_train, y_train, embedding_matrix = embedding_matrix, embedding_trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1000, 300)         3000300   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,160,801\n",
      "Trainable params: 160,501\n",
      "Non-trainable params: 3,000,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 16750 samples, validate on 8250 samples\n",
      "Epoch 1/3\n",
      "16750/16750 [==============================] - 1026s 61ms/step - loss: 0.6690 - accuracy: 0.5752 - val_loss: 0.6468 - val_accuracy: 0.6125\n",
      "Epoch 2/3\n",
      "16750/16750 [==============================] - 1052s 63ms/step - loss: 0.6441 - accuracy: 0.6199 - val_loss: 0.6502 - val_accuracy: 0.6028\n",
      "Epoch 3/3\n",
      "16750/16750 [==============================] - 931s 56ms/step - loss: 0.6156 - accuracy: 0.6480 - val_loss: 0.6134 - val_accuracy: 0.6480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2348f3277c8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lstm(X_train, y_train, embedding_matrix = embedding_matrix, embedding_trainable = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Removing the words not available in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import imdb\n",
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a hell of a time (infinite) to process\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "word2vec_vocab = list(word2vec.vocab.keys())\n",
    "X_removed = pd.Series(X_train).apply(lambda x: [id2word.get(i) for i in x if id2word.get(i) in word2vec_vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Using the custom embeddings (imdb dataset) for unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is this word not found in imdb dataset? Index: 3 , Word: a\n"
     ]
    }
   ],
   "source": [
    "# Loading our custom model(embedding) trained on imdb dataset\n",
    "word2vec_imdb = Word2Vec.load('word2vec_custom_imdb.bin')\n",
    "\n",
    "embedding_dim = 300\n",
    "word_index = imdb.get_word_index()\n",
    "embedding_matrix = np.zeros((vocabulary_size + 1, embedding_dim))\n",
    "id2word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "for i in range(1,vocabulary_size + 1):\n",
    "    word = id2word.get(i)\n",
    "    try: \n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "    except:  \n",
    "        try:\n",
    "            # if word not present in the dictionary - use custom trained embedding\n",
    "            embedding_matrix[i] = word2vec_imdb[word]\n",
    "        except:\n",
    "            print('Why is this word not found in imdb dataset?','Index:',i,', Word:',word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 1000, 300)         3000300   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,160,801\n",
      "Trainable params: 3,160,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Monish Kumar\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16750 samples, validate on 8250 samples\n",
      "Epoch 1/3\n",
      "16750/16750 [==============================] - 1895s 113ms/step - loss: 0.5481 - accuracy: 0.7073 - val_loss: 0.4810 - val_accuracy: 0.7724\n",
      "Epoch 2/3\n",
      "16750/16750 [==============================] - 1648s 98ms/step - loss: 0.3803 - accuracy: 0.8302 - val_loss: 0.4517 - val_accuracy: 0.8034\n",
      "Epoch 3/3\n",
      "16750/16750 [==============================] - 948s 57ms/step - loss: 0.3320 - accuracy: 0.8644 - val_loss: 0.3847 - val_accuracy: 0.8448\n"
     ]
    }
   ],
   "source": [
    "train_lstm(X_train, y_train, embedding_matrix = embedding_matrix, embedding_trainable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec['< UNK >'] - train them\n",
    "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
