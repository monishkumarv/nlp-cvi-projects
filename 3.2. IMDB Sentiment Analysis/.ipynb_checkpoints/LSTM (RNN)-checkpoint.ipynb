{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our local dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i have to differ from the other comments poste...\n",
       "1    i saw this movie with low expectations and was...\n",
       "2    taran adarsh a reputed critic praised such a d...\n",
       "3    when i first heard that the subject matter for...\n",
       "4    with the release of peter jackson's famed \"lor...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./imdb_small.csv')\n",
    "\n",
    "map_dict = {'negative':0,'positive':1}\n",
    "train =train.replace({'sentiment':map_dict})  \n",
    "\n",
    "train['review'] = train['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['review'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize only after doing all the necesarry preprocessing steps: (such as lower-casing, etc...)\n",
    "- Tokenizer - Numbers for each word is assigned based on their frequency. Most occuring word takes the value of 1. '0' is used for padding.\n",
    "- max_features = 10 - Only the top 9 (excluding '0' for padding) frequently occuring words are taken as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "\n",
    "import num2words\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def convertnum2words (sentence):\n",
    "    new_sentence = sentence\n",
    "    for i in sentence.split():\n",
    "        if i.isdigit():\n",
    "            sentence = sentence.replace(i,num2words.num2words(int(i)))\n",
    "    return sentence\n",
    "\n",
    "def countstopwords(sentence):\n",
    "    count = 0\n",
    "    for i in (sentence.split()):\n",
    "        if i in stop_words:\n",
    "            count +=1 \n",
    "    return count\n",
    "\n",
    "train['review'] = train['review'].apply(lambda x: x.replace('<br />','.'))\n",
    "train['review'] = train['review'].str.replace('[^\\w\\s]','')\n",
    "train['review'] = train['review'].apply(lambda x: convertnum2words(x))\n",
    "train['review'] = train['review'].str.replace('-',' ')\n",
    "train['review'] = train['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['review'] = train['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "train['review'] = train['review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 10000 # vocabulary_size\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, lower = True, split=' ')\n",
    "tokenizer.fit_on_texts(train['review'])\n",
    "\n",
    "x = tokenizer.texts_to_sequences(train['review'])\n",
    "\n",
    "# By default Pre-Sequence Padding is followed - if you want to use Post Padding use padding='post'\n",
    "x = pad_sequences(x) \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(len(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 753, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 225900)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               22590100  \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 25,590,201\n",
      "Trainable params: 25,590,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3350 samples, validate on 1650 samples\n",
      "Epoch 1/3\n",
      "3350/3350 [==============================] - 37s 11ms/step - loss: 4.0083 - accuracy: 0.5143 - val_loss: 0.6470 - val_accuracy: 0.6236\n",
      "Epoch 2/3\n",
      "3350/3350 [==============================] - 36s 11ms/step - loss: 0.1718 - accuracy: 0.9427 - val_loss: 0.4454 - val_accuracy: 0.7970\n",
      "Epoch 3/3\n",
      "3350/3350 [==============================] - 25s 7ms/step - loss: 0.0203 - accuracy: 0.9991 - val_loss: 0.4207 - val_accuracy: 0.8085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2e854ac8d48>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
    "\n",
    "embedding_size=300\n",
    "max_words = len(x[0])\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(x,train['sentiment'], test_size = 0.33, random_state = 123)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 753, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,160,501\n",
      "Trainable params: 3,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3350 samples, validate on 1650 samples\n",
      "Epoch 1/3\n",
      "3350/3350 [==============================] - 294s 88ms/step - loss: 0.6211 - accuracy: 0.6803 - val_loss: 0.4196 - val_accuracy: 0.8248\n",
      "Epoch 2/3\n",
      "3350/3350 [==============================] - 313s 93ms/step - loss: 0.2427 - accuracy: 0.9096 - val_loss: 0.4202 - val_accuracy: 0.8291\n",
      "Epoch 3/3\n",
      "3350/3350 [==============================] - 323s 96ms/step - loss: 0.0787 - accuracy: 0.9770 - val_loss: 0.5367 - val_accuracy: 0.8176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2e85484f288>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=300\n",
    "max_words = len(x[0])\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(x,train['sentiment'], test_size = 0.33, random_state = 123)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the IMDB dataset from keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Review': [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "\n",
      "'Label': 1\n"
     ]
    }
   ],
   "source": [
    "print(\"'Review':\",X_train[0])\n",
    "print(\"\\n'Label':\",y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Review': ['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'musicians', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'landed', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'frog', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'barrel', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'heartfelt', 'film', 'want', 'an']\n",
      "\n",
      "'Label': 1\n"
     ]
    }
   ],
   "source": [
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print(\"'Review':\",[id2word.get(i, ' ') for i in X_train[0]])\n",
    "print(\"\\n'Label':\",y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum review length: {}'.format(len(max((X_train + X_test), key=len))))\n",
    "print('Minimum review length: {}'.format(len(min((X_train + X_test), key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_words = 1000\n",
    "\n",
    "# By default Pre-Sequence Truncation is followed - if you want to use Post-Sequence Truncation use truncating='post'\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 1000, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 300000)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               30000100  \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 33,000,201\n",
      "Trainable params: 33,000,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 16750 samples, validate on 8250 samples\n",
      "Epoch 1/3\n",
      "16750/16750 [==============================] - 183s 11ms/step - loss: 1.3871 - accuracy: 0.7425 - val_loss: 0.3435 - val_accuracy: 0.8567\n",
      "Epoch 2/3\n",
      "16750/16750 [==============================] - 177s 11ms/step - loss: 0.0568 - accuracy: 0.9826 - val_loss: 0.3418 - val_accuracy: 0.8632\n",
      "Epoch 3/3\n",
      "16750/16750 [==============================] - 177s 11ms/step - loss: 0.0060 - accuracy: 0.9999 - val_loss: 0.3516 - val_accuracy: 0.8688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2e72af5ac08>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "embedding_size=300\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 1000, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,160,501\n",
      "Trainable params: 3,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 16750 samples, validate on 8250 samples\n",
      "Epoch 1/3\n",
      "16750/16750 [==============================] - 2310s 138ms/step - loss: 0.4737 - accuracy: 0.7832 - val_loss: 0.3613 - val_accuracy: 0.8469\n",
      "Epoch 2/3\n",
      "16750/16750 [==============================] - 2230s 133ms/step - loss: 0.2747 - accuracy: 0.8941 - val_loss: 0.3746 - val_accuracy: 0.8407\n",
      "Epoch 3/3\n",
      "16750/16750 [==============================] - 2442s 146ms/step - loss: 0.2163 - accuracy: 0.9180 - val_loss: 0.3405 - val_accuracy: 0.8707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2e713e0c948>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=300\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train,y_train, test_size = 0.33, random_state = 123)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
