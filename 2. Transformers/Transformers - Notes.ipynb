{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Transformers: Attention is All You Need!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- Nice explanation with illustration (Read fully and try to understand everything) - [link](https://www.michaelphi.com/illustrated-guide-to-transformers/)\n",
    "- Another Nice explanation with examples - [link](http://jalammar.github.io/illustrated-transformer/)\n",
    "- Transformers - [link](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)\n",
    "- You can also refer to the official paper: <b>Attention is all you need!</b>\n",
    "- Self Attention explained with example (illustration) - [link](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n",
    "\n",
    "- Attention and Transformers - [link](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/)\n",
    "\n",
    "- Stanford - Checkout first few slides (also try to search if the crresponding course video lecture is available_ - [link](https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>NOTE : </b><font color='purple'>The code you wrote is the <b>most basic and simplest Transformer architecture ever</b>. Please refer to the <b>[Hugging Face library](https://huggingface.co/transformers/index.html)</b> for advanced and complex Transformers with pretrained weights. this library also provides you with an option of fine-tuning the models (i.e. you can use the pretrained weights as initial weights and train a model on your own dataset if required - I think its not necessary for beginners)</font>\n",
    "<font color='darkblue'>\n",
    "- Refer  <b>'USING ü§ó TRANSFORMERS'</b>  section for general tutorials on how to use the huggingface library.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "Transformers - Recurrent Networks were, until now, one of the best ways to capture the timely dependencies in sequences. However, the team presenting the paper proved that an architecture with only attention-mechanisms without any RNN (Recurrent Neural Networks) can improve on the results in translation task and other tasks!\n",
    "\n",
    "- Simple NN - no context, no postion\n",
    "- Simple LSTM encoder-decoder - captures context (difficult for longer sentences) and position\n",
    "- Transformers - captures both context (not difficult for longer sentences) and position of words\n",
    "\n",
    "<img style=\"float: left;\" src=\"./assets/transformer3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we are not using any RNNs we wont be able to capture the order/position of words in a sentence. In order to capture that we use positional encoding.\n",
    "- <b>For a detailed explanation refer : [link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#what-is-positional-encoding-and-why-do-we-need-it-in-the-first-place) & [link](https://medium.com/swlh/elegant-intuitions-behind-positional-encodings-dc48b4a4a5d1)</b>\n",
    "- Given a sequence of words, we process into word embeddings Z ∑: N x h ∑, N represents the number of words in a sampled sequence, h ∑ represents the embedding size. Then, pos ‚àà [0, N-1] is the position of the word in the sequence and i ‚àà [0, h ∑-1] is the index which spans the dimension of the word embedding.\n",
    "\n",
    "\n",
    "#### ..................................................................................................................................................................................................................................................\n",
    "#### Given a word embeddings Z ∑: N x h ∑\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://miro.medium.com/max/507/1*NBetROvAUpwf3KH31-nAOg.png\">\n",
    "\n",
    "            - N: Number of word in the sequence\n",
    "            - h ∑: Dimension size of word embedding (d_model)\n",
    "            - pos: position of the current word in the sequence in [0, N-1]\n",
    "            - i: index of the dimensional index of word embedding in [0, h ∑-1]\n",
    "\n",
    "#### ..................................................................................................................................................................................................................................................\n",
    "#### For FAQs such as the following refer this [link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#what-is-positional-encoding-and-why-do-we-need-it-in-the-first-place) :\n",
    "\n",
    "- Why positional embeddings are summed with word embeddings instead of concatenation?\n",
    "- Doesn't the position information get vanished once it reaches the upper layers? (cant seem to undersatnd the question...)\n",
    "- Why are both sine and cosine used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Masking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Padding Mask:</b> used in all three Multi-head Attention blocks of the Transformers (since the inputs they get have padded values)\n",
    "- Now that all samples have a uniform length, the model must be informed that some part of the data is actually padding and should be ignored.\n",
    "- To ensure that this is done, we mask padded positions (setting them to `-inf`) before the softmax step in the self-attention calculation.\n",
    "- When these raw attention vaules are passed through a softmax layer, attention values at masked positions become '0' since `e^(-inf)` = 0\n",
    "\n",
    "<b> Look-ahead Mask:</b> used in the 2nd Multi-head Attention block of the Decoder layer \n",
    "- Lets say we have the following sequence as an input for our decoder: ‚ÄúI love it‚Äù, then the expected prediction for the token at position two (‚Äúlove‚Äù) is the token at the next position (‚Äúit‚Äù). We do not want the attention mechanism to share any information regarding the third token, when giving a prediction for the second token.\n",
    "- To ensure that this is done, we mask future positions (setting them to `-inf`) before the softmax step in the self-attention calculation.\n",
    "- When these raw attention vaules are passed through a softmax layer, attention values at masked positions become '0' since `e^(-inf)` = 0\n",
    "- the Transformer applies a mask to the input in the 1st Multi-head Attention module (of both Encoder and Decoder part) to avoid seeing potential ‚Äòfuture‚Äô sequence elements. <b>This is specific to the Transformer architecture because we do not have RNNs where we can input our sequence sequentially.</b> Here, we input everything together and if there were no mask, the multi-head attention would consider the whole decoder input sequence at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-head Attention:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Lets say your model (Multi-head Attention) has 4 heads and the dimension of the input matrix is `7x512` (sequence length = 7, embedding dim = 512). Now the input matrix is split <b>(copied/cloned)</b> into 4 parts each of dimension `7x128` and then these 4 matrices are fed into each heads which then outputs vectors each of dimension `7x128`. These outputs are then concatenated to form a matrix of dimension `7x512` and then passed through a <b>Linear layer</b>.</i>\n",
    "\n",
    "The type of attention that uses all the encoder hidden states is also known as <b><u>global attention</u>.</b> In contrast, <b><u>local attention</u></b> uses only a subset of the encoder hidden states. As the scope of this article (my notes) is global attention, any references made to ‚Äúattention‚Äù in this article are taken to mean ‚Äúglobal attention.‚Äù\n",
    "\n",
    "    - Q is a matrix that contains the query (vector representation of one word in the sequence)\n",
    "    - K are all the keys (vector representations of all the words in the sequence)\n",
    "    - V are the values, which are again the vector representations of all the words in the sequence. \n",
    "\n",
    "For the 1st Multi-head Attention module (of the Encoder and Decoder part), V is identical to Q. However, for the attention module that is taking into account the encoder and the decoder sequences (i.e. 2nd Multi-head Attention module of the Decoder part), V is different from the sequence represented by Q.\n",
    "\n",
    "### <u>Intuition behind Scaled dot product attention</u>:\n",
    "- For calculating attention weights why do we start with the dot-product of `Query` and `Key`\n",
    "- We know that `Query` is the vector representation of <b><u>one word in the sequence</u></b> and `Key` is the vector representations of <b><u>all the words in the sequence</u>.</b> So when you do dot-product of Query with each and every word of the Key, the resulting product (attention weights) will be nearly 1 for all those vectors of Keys which are identical to the Query vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>What is the use of each <u>Multi-head Attention</u> (MHA) block and the <u>Attention weights</u> obtained from the respaective blocks ?</i>\n",
    " - <b>`1st MHA block` :</b> Self attention - Shows how each word from input sequence is related/affected by other words in the sentence. The attention weights obtained are multiplied with the `Value` vector to <b>emphasize the important features</b> of the encoder input i.e. <b>The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words</b>\n",
    " - <b>`2nd MHA block` :</b> Self attention - Shows how each word from output sequence is related/affected by other words in the sentence. The attention weights obtained are multiplied with the `Value` vector to <b>emphasize the important features</b> of the encoder input i.e. <b>The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words</b>\n",
    " - <b>`3rd MHA block` :</b> Context Attention - The attention weights obtained here will be used for visualization. Because only this shows, in what way each word of the decoder (output sequence, i.e. translated word) is related/affected to each word from the encoder (input sequence).\n",
    "\n",
    "<i>If you are curious (or) want to check how the model performs, then you can save the attention weights obtained from the 1st and 2nd block to visualise the `Self Attention` of each word in a sentence.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Residual Connections:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The multi-headed attention block's output vector is added to the original positional input embedding. This is called a residual connection. The output of the residual connection then goes through a Normalization Layer.\n",
    "- <u>There is a high possibility that the position information will get vanished once it reaches the upper layers</u> (i.e. once the input passes Linear and Multi-haed Attention Layers). So in order to safely maintain the positional information of the vectors we add the original input (which contains the positional information) of each multi-headed attention blocks to the output of the respective multi-headed attention blocks üòä\n",
    "\n",
    "\n",
    "- During experimentation, the network displayed catastrophic results on removing the Residual Connections.\n",
    "- <b>[Article](https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09):</b> The authors have also discussed concatenation of the positional embeddings instead of adding them (ref: Allen NLP podcast). They were in the process of doing said experiments, but their initial results seem to say that the residual connections there can be mainly applied to the concatenated positional encoding section to propagate it through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Dropout (DropAttention):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this paper, we demonstrate the benefit of dropout in self-attention layers (DropAttention) with two key distinctions compared with the dropout used in fully-connected layers and recurrent layers. The first is that DropAttention randomly sets attention weights to zero, which can be interpreted as dropping a set of neurons along different dimensions.\n",
    "- <b>DropAttention aims to encourage the model to utilize the full context of the input sequences rather than relying solely on a small piece of features.</b>\n",
    "- For example, for sentiment classification, the prediction is usually dominated by one or several emotional words, ignoring other informative patterns. This can make the model overfit some specific patterns.\n",
    "\n",
    "\n",
    "- In fully-connected and recurrent layers, `Dropout` discourages the complex co-adaptation of different units in the same layer, while `DropAttention` prevents different contextualized feature vectors from co-adapting, learning features which are generally helpful for task-specific prediction. \n",
    "- Experiments on a wide range of tasks with different-scale datasets show that DropAttention can improve performance and reduce overfitting.\n",
    "\n",
    "<b>Reference: [link](https://www.groundai.com/project/dropattention-a-regularization-method-for-fully-connected-self-attention-networks/1)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How stacked layers of Encoder and decoder work:\n",
    "- The output of the 1st Encoder block is used as the input (Query, Key and Value) of the 2nd Encoder block and so on. \n",
    "- The same thing happens for the 1st Multi-head Attention of every decoder part present in the Decoder satck.\n",
    "- And for the 2nd Multi-head Attention part of every decoder in the Decoder stack, the final output of the last encoder of the Encoder stack is used as `Key` and `Value`, and the output of it's corresponding 1st Multi-head Attention is used as `Query`.\n",
    "\n",
    "\n",
    "- <b><i>Just like we did for simple encoder-decoder systems, we use the Encoder output (Key) and the Decoder output (Query) for calculating the attention weights. These attention weights are multiplied with the Encoder output to obtain the `context vector`.</i></b>\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/06/Screenshot-from-2019-06-17-20-03-14.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>LayerNormalization</i>\n",
    "- The layer normalizations are used to stabilize the network which results in substantially reducing the training time necessary. \n",
    "- Types and Uses of Normalization explained - https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- <b><u><i>Point-wise Feedforward Layer</i></u> :</b> The pointwise feedforward layer is used to project the attention outputs potentially giving it a richer representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>Why scaling is done in scaled dot-product?</i>\n",
    "- The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
    "of `1/‚àödk`.\n",
    "- Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
    "- While for small values of dk the two mechanisms perform similarly, <u>additive attention outperforms\n",
    "dot product attention without scaling for larger values of dk</u>. \n",
    "\n",
    "    <b><i>\" We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
    "extremely small gradients. \" </b></i>\n",
    "\n",
    "\n",
    "- <b><i>To counteract this effect, we scale the dot products by `1/‚àödk`</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u><i>Summary</i></u>:\n",
    "#### Inputs to the self-attention module:\n",
    "- Embedding module\n",
    "- Positional encoding\n",
    "- Truncating\n",
    "- Masking\n",
    "\n",
    "#### Adding more self-attention modules:\n",
    "- Multihead\n",
    "- Layer stacking\n",
    "\n",
    "#### Modules between self-attention modules:\n",
    "- Linear transformations \n",
    "- LayerNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of Transformers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/\n",
    "- https://medium.com/towards-artificial-intelligence/address-limitation-of-rnn-in-nlp-problems-by-using-transformer-xl-866d7ce1c8f4\n",
    "- https://www.quora.com/What-are-the-benefits-of-Transformers-over-LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to translate a French sentence without having the German sentence:\n",
    "The trick here is to re-feed our model for each position of the output sequence until we come across an end-of-sentence token.\n",
    "A more step by step method would be:\n",
    "Input the full encoder sequence (French sentence) and as decoder input, we take an empty sequence with only a start-of-sentence token on the first position. This will output a sequence where we will only take the first element.\n",
    "That element will be filled into second position of our decoder input sequence, which now has a start-of-sentence token and a first word/character in it.\n",
    "Input both the encoder sequence and the new decoder sequence into the model. Take the second element of the output and put it into the decoder input sequence.\n",
    "Repeat this until you predict an end-of-sentence token, which marks the end of the translation.\n",
    "We see that we need multiple runs through our model to translate our sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of self-attention:\n",
    "- ‚ÄùThe animal didn't cross the street because it was too tired‚Äù\n",
    "\n",
    "- What does ‚Äúit‚Äù in this sentence refer to? Is it referring to the street or to the animal? It‚Äôs a simple question to a human, but not as simple to an algorithm.\n",
    "\n",
    "- When the model is processing the word ‚Äúit‚Äù, self-attention allows it to associate ‚Äúit‚Äù with ‚Äúanimal‚Äù.\n",
    "\n",
    "- As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can <b>help lead to a better encoding for this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths only in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer (i.e., each word in a sequence do not interact with each other while travelling through the layers model of the model except in the self-attention layer - so we can compute the weights corresponding to each of these words induvidualy thus making  parallel computation possible)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubts:\n",
    "- one fo LSTM's limitation is that Transfer Learning never really worked.Unlike ConvNets, you cant train on a different dataset and use it for your problem statement. You have to train on a specific dataset every time.\n",
    "\n",
    "- In lstms (takes O(n^2) time) we can predict the next word only after predicting the previous word - so parallel computation is impossible (using gpu: 'Am i a joke to you?') - But with transformers you can do parallel computation - Even though the time required is O(n^2) for transformers, this time can be saved by using a gpu coz the input words can be passed in parallel unlike LSTM (input words are passed one by one).\n",
    " \n",
    "- with transformers - transfer learning is possible\n",
    "  \n",
    "- When LSTM over transformers?: when sentence length is long or infinite coz transformers take N^2 time.\n",
    " \n",
    "- Implementation code - hugging face - pre-trained transformer library\n",
    "\n",
    "- Attention weights - how are they used - (nem_heads x att_wts) ??\n",
    "\n",
    "- Cant we use the same LayerNorm everywhere - shoud they be unique??\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Functions & Uses of the Encoder layer: </b>All the operations of the Encoder Layer are to encode the input to a continuous representation with attention information. This will help the decoder focus on the appropriate words in the input during the decoding process. <u>You can stack the encoder N times to further encode the information, where each layer has the opportunity to learn different attention representations therefore potentially boosting the predictive power of the transformer network</u>.\n",
    "\n",
    "- The output of the first multi-headed attention is a masked output vector with information on how the model should attend on the decoder's input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='darkblue'><u><i>Sample Example</i></u>:\n",
    "<i>Sentiment Analysis using a pretrained model pipeline from hugging face library...</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:transformers.modeling_tf_utils:All model checkpoint weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "WARNING:transformers.modeling_tf_utils:All the weights of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from transformers import pipeline\n",
    "\n",
    "# You need internet connectivity to run this print statement (dont worry, consumes neglible amount of data only...)\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]\n"
     ]
    }
   ],
   "source": [
    "print(classifier('I hate you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9738867878913879}]\n"
     ]
    }
   ],
   "source": [
    "print(classifier('I saw the movie bad boys for life'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]\n"
     ]
    }
   ],
   "source": [
    "print(classifier('I love you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.5864599347114563}]\n"
     ]
    }
   ],
   "source": [
    "print(classifier('did you go to work'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
