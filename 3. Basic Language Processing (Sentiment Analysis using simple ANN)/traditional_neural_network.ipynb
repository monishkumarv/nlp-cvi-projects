{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "        \n",
    "        \n",
    "train_data = pd.read_csv(r\"C:\\Users\\Monish Kumar\\Python projects\\Basic Language Processing\\nltk_input_data\\train_data.csv\")\n",
    "test_data = pd.read_csv(r\"C:\\Users\\Monish Kumar\\Python projects\\Basic Language Processing\\nltk_input_data\\test_data.csv\")\n",
    "\n",
    "train_data = train_data.drop(columns=['Unnamed: 0'])\n",
    "test_data = test_data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization constant...keep_prob\n",
    "# layer_1 = tf.nn.relu(layer_1)...\n",
    "# layer_1 = tf.nn.dropout(layer_1,keep_prob)\n",
    "\n",
    "# sigmoid instead of relu??...bias working and why is it used?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(train_data.drop(columns=['pos','neg']))\n",
    "Y_train = pd.DataFrame(train_data[['pos','neg']])\n",
    "\n",
    "X_test = pd.DataFrame(test_data.drop(columns=['pos','neg']))\n",
    "Y_test = pd.DataFrame(test_data[['pos','neg']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_1 = 500\n",
    "nodes_2 = 500\n",
    "nodes_3 = 500\n",
    "class_size = 2      # positive and negative\n",
    "feature_size = 423  # length of lexicon\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder('float',[None,feature_size])  # no of rows is not specified\n",
    "y = tf.placeholder('float')                      # no of rows and columns are not specified\n",
    "training_output = tf.placeholder('float',[None,class_size])\n",
    "submission_output = tf.placeholder('float',[None,class_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(input_data):\n",
    "    \n",
    "    hidden_layer_1 = {'weights':tf.Variable(tf.random.uniform([feature_size,nodes_1])),\n",
    "                      'bias':tf.Variable(tf.random.uniform([nodes_1]))}\n",
    "    hidden_layer_2 = {'weights':tf.Variable(tf.random.uniform([nodes_1,nodes_2])),\n",
    "                          'bias':tf.Variable(tf.random.uniform([nodes_2]))}\n",
    "    hidden_layer_3 = {'weights':tf.Variable(tf.random.uniform([nodes_2,nodes_3])),\n",
    "                          'bias':tf.Variable(tf.random.uniform([nodes_3]))}\n",
    "    output_layer = {'weights':tf.Variable(tf.random.uniform([nodes_3,class_size])),\n",
    "                          'bias':tf.Variable(tf.random.uniform([class_size]))}\n",
    "    \n",
    "    # (data * weights) + bias\n",
    "    l1 = tf.add(tf.linalg.matmul(input_data,hidden_layer_1['weights']),hidden_layer_1['bias'])\n",
    "    l1 = tf.nn.relu(l1)  # rectified linear function...similar to sigmoid function(tf.nn.sigmoid())\n",
    "    \n",
    "    l2 = tf.add(tf.linalg.matmul(l1,hidden_layer_2['weights']),hidden_layer_2['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.linalg.matmul(l2,hidden_layer_3['weights']),hidden_layer_3['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.add(tf.linalg.matmul(l3,output_layer['weights']),output_layer['bias'])\n",
    "    output = tf.nn.relu(output)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network():\n",
    "    \n",
    "    # Defining the graph for model...\n",
    "    prediction = neural_network(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y,logits = prediction))\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "    \n",
    "    # Defining graph for accuracy calculation...\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction ,'float'))\n",
    "        \n",
    "    epochs_num = 10\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(epochs_num):\n",
    "            epoch_loss = 0\n",
    "            for n in range(int(X_train.shape[0]/batch_size)):\n",
    "                epoch_x = X_train.loc[(n*batch_size):((n+1)*batch_size)]\n",
    "                epoch_y = Y_train.loc[(n*batch_size):((n+1)*batch_size)]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x:epoch_x, y:epoch_y})\n",
    "                epoch_loss += c\n",
    "            print('Epoch',epoch+1,'completed out of',epochs_num,'loss: ',epoch_loss)\n",
    "            \n",
    "        \n",
    "        print('\\nAccuracy on Training data : ',accuracy.eval(feed_dict = {x:X_train, y:Y_train})) \n",
    "        print('Accuracy on Testing data : ',accuracy.eval(feed_dict = {x:X_test, y:Y_test})) \n",
    "        \n",
    "        #training_output = sess.run(tf.argmax(prediction,1),feed_dict = {x:X_train})   # using sess.run() ...but both produces the same result\n",
    "        #training_output = tf.argmax(prediction,1).eval(feed_dict = {x:X_train})        # using .eval() ...both can only work under with tf.Session() as sess:\n",
    "        #submission_output = sess.run(tf.argmax(prediction,1),feed_dict = {x:X_test})\n",
    "        \n",
    "        print('\\ndone...')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-41e79fa71756>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch 1 completed out of 10 loss:  2742644.6991271973\n",
      "Epoch 2 completed out of 10 loss:  1910092.4030456543\n",
      "Epoch 3 completed out of 10 loss:  1196826.8077392578\n",
      "Epoch 4 completed out of 10 loss:  872371.3898773193\n",
      "Epoch 5 completed out of 10 loss:  897752.4040298462\n",
      "Epoch 6 completed out of 10 loss:  620831.3835449219\n",
      "Epoch 7 completed out of 10 loss:  703831.4662017822\n",
      "Epoch 8 completed out of 10 loss:  461216.32232666016\n",
      "Epoch 9 completed out of 10 loss:  368526.47956085205\n",
      "Epoch 10 completed out of 10 loss:  337104.1943664551\n",
      "\n",
      "Accuracy on Training data :  0.50257915\n",
      "Accuracy on Testing data :  0.48968107\n",
      "\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "# train and display loss and accuracy\n",
    "train_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "lexicon_data = pd.read_csv(r\"C:\\Users\\Monish Kumar\\Python projects\\Basic Language Processing\\nltk_input_data\\lexicon_data.csv\")\n",
    "lexicon = list(lexicon_data['0'])\n",
    "\n",
    "t = 'you are a not a  fuck fuck fuck fuck fuck boy'\n",
    "t = t.lower()\n",
    "words = word_tokenize(t)\n",
    "#print('Before lemmatization -',words)\n",
    "\n",
    "lemmed_words = []\n",
    "for i in words:\n",
    "    lemmed_words.append(WordNetLemmatizer().lemmatize(i)) \n",
    "#print('After lemmatization -',lemmed_words)\n",
    "\n",
    "#print(Counter(lemmed_words))\n",
    "\n",
    "input_demo = np.zeros(len(lexicon))\n",
    "for w in lemmed_words:\n",
    "    if w in lexicon:\n",
    "        pos = lexicon.index(w)\n",
    "        input_demo[pos] += 1\n",
    "#print('Sample Input data -',input_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Statement\n"
     ]
    }
   ],
   "source": [
    "pred = neural_network(tf.cast(pd.DataFrame(input_demo.reshape(1,-1)),'float'))\n",
    "out = tf.argmax(pred,1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    p,o = sess.run([pred,out])\n",
    "    \n",
    "if o == 0:\n",
    "    print('Positive Statement')\n",
    "else:\n",
    "    print('Negative Statement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being a positive Statement - 0.49 %\n",
      "Probability of being a negative Statement - 0.51 %\n"
     ]
    }
   ],
   "source": [
    "print ('Probability of being a positive Statement -',round(p[0,0]/(p[0,0] + p[0,1]),2),'%')\n",
    "print ('Probability of being a negative Statement -',round(p[0,1]/(p[0,0] + p[0,1]),2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial using 1.6 million samples of data - https://www.youtube.com/watch?v=JeamFbHhmDo&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=51  \n",
    "If you see the last 3 min of the above video you will notice that the even though we used bigger amount of dataset we only reached an accuracy of ~74%\n",
    "### Conclusion\n",
    "Natural language processing doesn't perform well on traditional feed-forward back-prob neural networks. It may perform well on Recurrent Neural Networks (or) LSTMs (or) other algos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
