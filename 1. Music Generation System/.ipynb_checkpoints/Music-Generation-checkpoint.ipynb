{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEI6gbW9Xh2t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import *\n",
    "\n",
    "from music21 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99F2Mtptj4MS"
   },
   "outputs": [],
   "source": [
    "data_directory = \"./dataset\"\n",
    "data_file = \"Data_Tunes.txt\"\n",
    "charIndex_json = \"char_to_index.json\"\n",
    "model_weights_directory = './model_weights/'\n",
    "\n",
    "# Train configurations...\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LENGTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_XacfcrSX3OK"
   },
   "source": [
    "### ABC notation of Music\n",
    "There are two parts in ABC-notation.\n",
    "- Part-1 represents meta data. Lines in the Part-1 of the tune notation, beginning with a letter followed by a colon, indicate various aspects of the tune such as the index, when there are more than one tune in a file (X:), the title (T:), the time signature (M:), the default note length (L:), the type of tune (R:) and the key (K:).\n",
    "- Part-2 represents the tune, which is a sequence of characters where each character represents some musical note.\n",
    "\n",
    "<img style=\"float: centre;\" src=\"./assets/abc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kU9kGmh6RKyI"
   },
   "outputs": [],
   "source": [
    "# Function to remove Part-1 of  and append a start token i.e 'Z' for each tune in the dataset\n",
    "def preprocess(data):\n",
    "    list1=list(data)\n",
    "    list2=['\\n','\\n','\\n']\n",
    "    ignore=['X','T','M','S','K','P']\n",
    "    \n",
    "    i=0\n",
    "    #to remove Part1:\n",
    "    while(i<len(list1)):\n",
    "    if(((list1[i] in ignore) and (list1[i+1]==\":\"))or list1[i]=='%' ):\n",
    "        del list2[-1]\n",
    "        while(list1[i]!='\\n'):\n",
    "            i=i+1\n",
    "    list2.append(list1[i])\n",
    "    i=i+1\n",
    "    \n",
    "    i=0\n",
    "    #to append 'Z'(start token)\n",
    "    preprocess_data=[]\n",
    "    \n",
    "    while(i<len(list2)):\n",
    "    if(list2[i]=='\\n'and list2[i+1]=='\\n' and list2[i+2]=='\\n'):\n",
    "        preprocess_data.append('Z')\n",
    "        i=i+3\n",
    "    else:\n",
    "        preprocess_data.append(list2[i])\n",
    "        i=i+1\n",
    "    \n",
    "    return preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYOozwWeDhTv"
   },
   "outputs": [],
   "source": [
    "# Function to create \"char_to_index\" and \"index_to_char\" dictionaries so as to map each character to an index and vice versa.\n",
    "\n",
    "# Returns \"all_characters_as_indices\" i.e an array containing all characters of the dataset replaced with their corresponding \n",
    "# indices as per the vocabulary along with \"num_unique_chars\" i.e an integer equal to number of unique characters in the data.\n",
    "\n",
    "def read_data(preprocess_data):\n",
    "    char_to_index = {ch: i for (i, ch) in enumerate(sorted(list(set(preprocess_data))))}\n",
    "\n",
    "    with open(os.path.join(data_directory, charIndex_json), mode = \"w\") as f:\n",
    "        json.dump(char_to_index, f)\n",
    "\n",
    "    index_to_char = {i: ch for (ch, i) in char_to_index.items()}\n",
    "    num_unique_chars = len(char_to_index)\n",
    "\n",
    "    all_characters_as_indices = np.asarray([char_to_index[c] for c in preprocess_data], dtype = np.int32)\n",
    "\n",
    "    return all_characters_as_indices, num_unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FWErp9l_c1y"
   },
   "outputs": [],
   "source": [
    "# Function which prepares dataset for training\n",
    "# Returns X and Y which will be used as input and target output for training the model.\n",
    "\n",
    "def input_output(all_chars_as_indices,num_unique_chars):\n",
    "    \n",
    "    total_length = all_chars_as_indices.shape[0]\n",
    "    num_examples = int(total_length/SEQ_LENGTH)\n",
    "    \n",
    "    X = np.zeros((num_examples,SEQ_LENGTH))\n",
    "    Y = np.zeros((num_examples,SEQ_LENGTH,num_unique_chars))\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        for j in range(SEQ_LENGTH):\n",
    "            X[i,j] = all_chars_as_indices[i*SEQ_LENGTH+j]\n",
    "            Y[i,j,all_chars_as_indices[i*SEQ_LENGTH+j+1]] = 1\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hDNI8EFAwran",
    "outputId": "d706902f-7bcc-4add-dbe8-f00338ecf25f"
   },
   "source": [
    "### <u>Training model architecture</u>:\n",
    "<img style=\"float: left;\" src=\"./assets/build.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0H5nVWJv_ihC"
   },
   "outputs": [],
   "source": [
    "# Function to build the training model\n",
    "def build_model( seq_length, num_unique_chars):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim = num_unique_chars, output_dim = 512, input_shape = (seq_length,))) \n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(num_unique_chars)))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Prediction model architecture</u>: (for generating music)\n",
    "<img style=\"float: left;\" src=\"./assets/generate.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bF_Z6KIbqw2"
   },
   "source": [
    "<b>stateful:</b> If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGgrLrX4BnHh"
   },
   "outputs": [],
   "source": [
    "# Function which builds model for generating (prediction) music sequences.\n",
    "def make_model(num_unique_chars):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # batch_input_shape is = 1, coz for prediction starting index (Z) is given as input...\n",
    "    model.add(Embedding(input_dim = num_unique_chars, output_dim = 512, batch_input_shape = (1, 1))) \n",
    "  \n",
    "    # stateful: If True, the last state for each sample at index i in a batch will be used \n",
    "    # as initial state for the sample of index i in the following batch.\n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(256,return_sequences=True, stateful = True)) \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add((Dense(num_unique_chars)))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "yI3ceWwF_XL6",
    "outputId": "e2d1a356-5176-4b18-93d0-266a742127c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of preprocess_data-116963\n",
      "vocab_size=59\n",
      "all_characters=[33 44 57 ... 15 20 57]\n",
      "length of all_characters-116963\n",
      "shape of X=(1827, 64)\n",
      "shape of Y=(1827, 64, 59)\n"
     ]
    }
   ],
   "source": [
    "# Load data:\n",
    "file = open(os.path.join(data_directory, data_file), mode = 'r')\n",
    "data = file.read()\n",
    "file.close()\n",
    "\n",
    "# Pre-process and create train data:\n",
    "preprocess_data = preprocess(data)\n",
    "all_characters_as_indices, num_unique_chars = read_data(preprocess_data)\n",
    "X, Y = input_output(all_characters_as_indices,num_unique_chars)\n",
    "\n",
    "# stats...\n",
    "print(\"length of preprocess_data - {}\".format(len(preprocess_data)))\n",
    "print(\"vocab_size = {}\".format(num_unique_chars))\n",
    "print(\"all_characters = {}\".format(all_characters_as_indices))\n",
    "print(\"length of all_characters - {}\".format(len(all_characters_as_indices)))\n",
    "print(\"shape of X = {}\".format(X.shape))\n",
    "print(\"shape of Y = {}\".format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_TNCaeqyAda9",
    "outputId": "174462fc-4e1d-424c-c0b6-a743cdfc7bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 512)           30208     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64, 256)           787456    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 64, 59)            15163     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 59)            0         \n",
      "=================================================================\n",
      "Total params: 1,883,451\n",
      "Trainable params: 1,883,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Monish Kumar\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1827/1827 [==============================] - 81s 44ms/step - loss: 3.0893 - accuracy: 0.1842\n",
      "Epoch 2/80\n",
      "1827/1827 [==============================] - 77s 42ms/step - loss: 2.2744 - accuracy: 0.3594\n",
      "Epoch 3/80\n",
      "1827/1827 [==============================] - 68s 37ms/step - loss: 1.8975 - accuracy: 0.4380\n",
      "Epoch 4/80\n",
      "1827/1827 [==============================] - 59s 33ms/step - loss: 1.7478 - accuracy: 0.4649\n",
      "Epoch 5/80\n",
      "1827/1827 [==============================] - 49s 27ms/step - loss: 1.6257 - accuracy: 0.4852\n",
      "Epoch 6/80\n",
      "1827/1827 [==============================] - 59s 32ms/step - loss: 1.5519 - accuracy: 0.5028\n",
      "Epoch 7/80\n",
      "1827/1827 [==============================] - 53s 29ms/step - loss: 1.4985 - accuracy: 0.5157\n",
      "Epoch 8/80\n",
      "1827/1827 [==============================] - 62s 34ms/step - loss: 1.4525 - accuracy: 0.5292\n",
      "Epoch 9/80\n",
      "1827/1827 [==============================] - 59s 32ms/step - loss: 1.4033 - accuracy: 0.5445\n",
      "Epoch 10/80\n",
      "1827/1827 [==============================] - 52s 28ms/step - loss: 1.3645 - accuracy: 0.5547\n",
      "Epoch 11/80\n",
      "1827/1827 [==============================] - 61s 33ms/step - loss: 1.3294 - accuracy: 0.5660\n",
      "Epoch 12/80\n",
      "1827/1827 [==============================] - 67s 36ms/step - loss: 1.2996 - accuracy: 0.5754\n",
      "Epoch 13/80\n",
      "1827/1827 [==============================] - 71s 39ms/step - loss: 1.2751 - accuracy: 0.5827\n",
      "Epoch 14/80\n",
      "1827/1827 [==============================] - 71s 39ms/step - loss: 1.2479 - accuracy: 0.5910\n",
      "Epoch 15/80\n",
      "1827/1827 [==============================] - 69s 38ms/step - loss: 1.2246 - accuracy: 0.5972\n",
      "Epoch 16/80\n",
      "1827/1827 [==============================] - 70s 38ms/step - loss: 1.2033 - accuracy: 0.6033\n",
      "Epoch 17/80\n",
      "1827/1827 [==============================] - 79s 43ms/step - loss: 1.1836 - accuracy: 0.6100\n",
      "Epoch 18/80\n",
      "1827/1827 [==============================] - 79s 43ms/step - loss: 1.1681 - accuracy: 0.6138\n",
      "Epoch 19/80\n",
      "1827/1827 [==============================] - 78s 42ms/step - loss: 1.1467 - accuracy: 0.6214\n",
      "Epoch 20/80\n",
      "1827/1827 [==============================] - 78s 43ms/step - loss: 1.1297 - accuracy: 0.6242\n",
      "Epoch 21/80\n",
      "1827/1827 [==============================] - 77s 42ms/step - loss: 1.1117 - accuracy: 0.6316\n",
      "Epoch 22/80\n",
      "1827/1827 [==============================] - 77s 42ms/step - loss: 1.0935 - accuracy: 0.6369\n",
      "Epoch 23/80\n",
      "1827/1827 [==============================] - 77s 42ms/step - loss: 1.0790 - accuracy: 0.6408\n",
      "Epoch 24/80\n",
      "1827/1827 [==============================] - 74s 41ms/step - loss: 1.0611 - accuracy: 0.6461\n",
      "Epoch 25/80\n",
      "1827/1827 [==============================] - 76s 42ms/step - loss: 1.0491 - accuracy: 0.6505\n",
      "Epoch 26/80\n",
      "1827/1827 [==============================] - 76s 42ms/step - loss: 1.0335 - accuracy: 0.6554\n",
      "Epoch 27/80\n",
      "1827/1827 [==============================] - 77s 42ms/step - loss: 1.0212 - accuracy: 0.6594\n",
      "Epoch 28/80\n",
      "1827/1827 [==============================] - 78s 42ms/step - loss: 1.0076 - accuracy: 0.6627\n",
      "Epoch 29/80\n",
      "1827/1827 [==============================] - 78s 43ms/step - loss: 0.9909 - accuracy: 0.6689\n",
      "Epoch 30/80\n",
      "1827/1827 [==============================] - 79s 43ms/step - loss: 0.9797 - accuracy: 0.6732\n",
      "Epoch 31/80\n",
      "1827/1827 [==============================] - 78s 43ms/step - loss: 0.9618 - accuracy: 0.6788\n",
      "Epoch 32/80\n",
      "1827/1827 [==============================] - 78s 43ms/step - loss: 0.9544 - accuracy: 0.6798\n",
      "Epoch 33/80\n",
      "1827/1827 [==============================] - 79s 43ms/step - loss: 0.9390 - accuracy: 0.6866\n",
      "Epoch 34/80\n",
      "1827/1827 [==============================] - 76s 42ms/step - loss: 0.9277 - accuracy: 0.6883\n",
      "Epoch 35/80\n",
      "1827/1827 [==============================] - 76s 42ms/step - loss: 0.9135 - accuracy: 0.6925\n",
      "Epoch 36/80\n",
      "1827/1827 [==============================] - 77s 42ms/step - loss: 0.9001 - accuracy: 0.6977\n",
      "Epoch 37/80\n",
      "1827/1827 [==============================] - 74s 41ms/step - loss: 0.8867 - accuracy: 0.7017\n",
      "Epoch 38/80\n",
      "1827/1827 [==============================] - 74s 41ms/step - loss: 0.8735 - accuracy: 0.7064\n",
      "Epoch 39/80\n",
      "1827/1827 [==============================] - 71s 39ms/step - loss: 0.8613 - accuracy: 0.7115\n",
      "Epoch 40/80\n",
      "1827/1827 [==============================] - 74s 41ms/step - loss: 0.8467 - accuracy: 0.7143\n",
      "Epoch 41/80\n",
      "1827/1827 [==============================] - 72s 39ms/step - loss: 0.8376 - accuracy: 0.7169\n",
      "Epoch 42/80\n",
      "1827/1827 [==============================] - 89s 49ms/step - loss: 0.8239 - accuracy: 0.7224\n",
      "Epoch 43/80\n",
      "1827/1827 [==============================] - 94s 52ms/step - loss: 0.8128 - accuracy: 0.7273\n",
      "Epoch 44/80\n",
      "1827/1827 [==============================] - 89s 49ms/step - loss: 0.7999 - accuracy: 0.7304\n",
      "Epoch 45/80\n",
      "1827/1827 [==============================] - 81s 45ms/step - loss: 0.7877 - accuracy: 0.7332\n",
      "Epoch 46/80\n",
      "1827/1827 [==============================] - 77s 42ms/step - loss: 0.7742 - accuracy: 0.7395\n",
      "Epoch 47/80\n",
      "1827/1827 [==============================] - 73s 40ms/step - loss: 0.7618 - accuracy: 0.7430\n",
      "Epoch 48/80\n",
      "1827/1827 [==============================] - 75s 41ms/step - loss: 0.7554 - accuracy: 0.7435\n",
      "Epoch 49/80\n",
      "1827/1827 [==============================] - 84s 46ms/step - loss: 0.7397 - accuracy: 0.7497\n",
      "Epoch 50/80\n",
      "1827/1827 [==============================] - 89s 49ms/step - loss: 0.7313 - accuracy: 0.7538\n",
      "Epoch 51/80\n",
      "1827/1827 [==============================] - 70s 38ms/step - loss: 0.7213 - accuracy: 0.7554\n",
      "Epoch 52/80\n",
      "1827/1827 [==============================] - 70s 38ms/step - loss: 0.7118 - accuracy: 0.7582\n",
      "Epoch 53/80\n",
      "1827/1827 [==============================] - 70s 38ms/step - loss: 0.6983 - accuracy: 0.7636\n",
      "Epoch 54/80\n",
      "1827/1827 [==============================] - 71s 39ms/step - loss: 0.6851 - accuracy: 0.7680\n",
      "Epoch 55/80\n",
      "1827/1827 [==============================] - 69s 38ms/step - loss: 0.6808 - accuracy: 0.7682\n",
      "Epoch 56/80\n",
      "1827/1827 [==============================] - 69s 38ms/step - loss: 0.6699 - accuracy: 0.7739\n",
      "Epoch 57/80\n",
      "1827/1827 [==============================] - 69s 38ms/step - loss: 0.6584 - accuracy: 0.7750\n",
      "Epoch 58/80\n",
      "1827/1827 [==============================] - 69s 38ms/step - loss: 0.6464 - accuracy: 0.7800\n",
      "Epoch 59/80\n",
      "1827/1827 [==============================] - 71s 39ms/step - loss: 0.6390 - accuracy: 0.7829\n",
      "Epoch 60/80\n",
      "1827/1827 [==============================] - 71s 39ms/step - loss: 0.6261 - accuracy: 0.7865\n",
      "Epoch 61/80\n",
      "1827/1827 [==============================] - 71s 39ms/step - loss: 0.6244 - accuracy: 0.7877\n",
      "Epoch 62/80\n",
      "1827/1827 [==============================] - 92s 51ms/step - loss: 0.6149 - accuracy: 0.7901\n",
      "Epoch 63/80\n",
      "1827/1827 [==============================] - 91s 50ms/step - loss: 0.6037 - accuracy: 0.7947\n",
      "Epoch 64/80\n",
      "1827/1827 [==============================] - 55s 30ms/step - loss: 0.5946 - accuracy: 0.7962\n",
      "Epoch 65/80\n",
      "1827/1827 [==============================] - 59s 32ms/step - loss: 0.5880 - accuracy: 0.7991\n",
      "Epoch 66/80\n",
      "1827/1827 [==============================] - 54s 30ms/step - loss: 0.5781 - accuracy: 0.8023\n",
      "Epoch 67/80\n",
      "1827/1827 [==============================] - 53s 29ms/step - loss: 0.5680 - accuracy: 0.8062\n",
      "Epoch 68/80\n",
      "1827/1827 [==============================] - 54s 29ms/step - loss: 0.5597 - accuracy: 0.8090\n",
      "Epoch 69/80\n",
      "1827/1827 [==============================] - 53s 29ms/step - loss: 0.5539 - accuracy: 0.8104\n",
      "Epoch 70/80\n",
      "1827/1827 [==============================] - 53s 29ms/step - loss: 0.5497 - accuracy: 0.8120\n",
      "Epoch 71/80\n",
      "1827/1827 [==============================] - 52s 29ms/step - loss: 0.5415 - accuracy: 0.8150\n",
      "Epoch 72/80\n",
      "1827/1827 [==============================] - 55s 30ms/step - loss: 0.5329 - accuracy: 0.8181\n",
      "Epoch 73/80\n",
      "1827/1827 [==============================] - 54s 30ms/step - loss: 0.5250 - accuracy: 0.8197\n",
      "Epoch 74/80\n",
      "1827/1827 [==============================] - 55s 30ms/step - loss: 0.5195 - accuracy: 0.8220\n",
      "Epoch 75/80\n",
      "1827/1827 [==============================] - 61s 33ms/step - loss: 0.5131 - accuracy: 0.8246\n",
      "Epoch 76/80\n",
      "1827/1827 [==============================] - 73s 40ms/step - loss: 0.5038 - accuracy: 0.8269\n",
      "Epoch 77/80\n",
      "1827/1827 [==============================] - 79s 43ms/step - loss: 0.5005 - accuracy: 0.8277\n",
      "Epoch 78/80\n",
      "1827/1827 [==============================] - 79s 43ms/step - loss: 0.4899 - accuracy: 0.8318\n",
      "Epoch 79/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1827/1827 [==============================] - 75s 41ms/step - loss: 0.4874 - accuracy: 0.8312\n",
      "Epoch 80/80\n",
      "1827/1827 [==============================] - 77s 42ms/step - loss: 0.4811 - accuracy: 0.8344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x256ed7dd908>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training:\n",
    "model = build_model(SEQ_LENGTH,num_unique_chars)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "checkpoint = ModelCheckpoint(filepath='./model_weights/weights.{epoch:02d}.hdf5', monitor='loss',\n",
    "                             save_best_only=True, save_weights_only=True, period=1)\n",
    "\n",
    "model.fit(X, Y, batch_size=16, epochs=80, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate music:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iw446L1OBzzl"
   },
   "outputs": [],
   "source": [
    "# Function which generates music sequences of length = gen_seq_length.\n",
    "def generate_sequence(gen_seq_length):\n",
    "    \n",
    "    # Open saved char_to_index dictionary (vocabulary)\n",
    "    with open(os.path.join(data_directory, charIndex_json)) as f:\n",
    "        char_to_index = json.load(f)\n",
    "    \n",
    "    index_to_char = {i:ch for ch, i in char_to_index.items()}\n",
    "    num_unique_chars = len(index_to_char)\n",
    "    \n",
    "    model = make_model(num_unique_chars)\n",
    "    model.load_weights(\"./model_weights/weights.80.hdf5\")\n",
    "     \n",
    "    sequence_index = [char_to_index['Z']]\n",
    "\n",
    "    # start music generation using the previously predicted notes: predicts the i'th using previous i-1 predictions...\n",
    "    for _ in range(gen_seq_length):\n",
    "        batch = np.zeros((1, 1))\n",
    "        batch[0, 0] = sequence_index[-1]\n",
    "        \n",
    "        # predict probablity of next note...\n",
    "        predicted_probs = model.predict_on_batch(batch).ravel()\n",
    "        \n",
    "        # Generates new song everytime due to this randomness\n",
    "        sample = np.random.choice(range(num_unique_chars), size = 1, p = predicted_probs)\n",
    "        sequence_index.append(sample[0])\n",
    "    \n",
    "    seq = ''.join(index_to_char[c] for c in sequence_index)\n",
    "    seq = 'M:6/8\\n' + str(seq)\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kveYEr8HCtcZ"
   },
   "outputs": [],
   "source": [
    "# Function to create a midi file given a music sequence in abc notation.\n",
    "def convert_to_midi(abc):\n",
    "    c = converter.subConverters.ConverterABC()\n",
    "    c.registerOutputExtensions = (\"midi\", )\n",
    "    c.parseData(abc)\n",
    "    s = c.stream\n",
    "    s.write('midi', fp='machine-generated-music.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "iWz5iH8oj3yf",
    "outputId": "bbd5e890-72b4-443d-f5f5-81a6324acc99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MUSIC SEQUENCE GENERATED: \n",
      "M:6/8\n",
      "ZL:1/8\n",
      "R:Hornpipe\n",
      "G2|:\"C\"c2ed c2GB|\"C\"c2GF E/2C3/2CE|\"D\"DFDF D2gf|\"C\"edef \"D7\"g2ec|\"G\"d2B2 G2:|\n",
      "ga|\"Em\"b^gbg e2fe|\"D\"d^cdA FAdf|\"Em\"edcB \"A7\"A2ag|\"D\"fgaf \"A7/e\"efed|\"A7\"cBcA \"D\"d2:|ZL:1/8\n",
      "R:Hor\n"
     ]
    }
   ],
   "source": [
    "# Generate music using the trained model...\n",
    "new_music = generate_sequence(192)\n",
    "print(\"\\nMUSIC SEQUENCE GENERATED: \\n{}\".format(new_music))\n",
    "convert_to_midi(new_music)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MusGenfinal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
