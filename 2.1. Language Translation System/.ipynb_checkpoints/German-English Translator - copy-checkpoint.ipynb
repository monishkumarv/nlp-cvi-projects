{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German-English Translator: Using Seq2seq modelling with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    ifile = open(filepath, mode='rt', encoding='utf-8')\n",
    "    data = ifile.read()\n",
    "    ifile.close()\n",
    "    \n",
    "    data = data.split('\\n')\n",
    "    data = [ i.split('\\t') for i in data ]\n",
    "    data = [ [i[0],i[1]] for i in data ]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_germ = load_data('./datasets/german-eng-translation.txt')\n",
    "eng_germ = pd.DataFrame(eng_germ, columns=['english','german'])\n",
    "\n",
    "eng_germ = eng_germ.iloc[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(line):\n",
    "    line = unicode_to_ascii(line.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    line = re.sub(r\"([?.!,¿])\", r\" \\1 \", line)\n",
    "    line = re.sub(r'[\" \"]+', \" \", line)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    line = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \",line)\n",
    "\n",
    "    line = line.strip()\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Max sentence length (German): 18')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert text to lowercase\n",
    "eng_germ['english'] = eng_germ['english'].apply(lambda x: preprocess_sentence(x))\n",
    "eng_germ['german'] = eng_germ['german'].apply(lambda x: preprocess_sentence(x))\n",
    "\n",
    "# Adding '<start>' and '<end>' tokens to target variable (English)\n",
    "eng_germ['english'] = eng_germ['english'].apply(lambda x: ('<BOS> ' + x + ' <EOS>'))\n",
    "\n",
    "# Visualising sentence lengths\n",
    "len_eng = eng_germ['english'].apply(lambda x: len(x.split()))\n",
    "len_germ = eng_germ['german'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1)\n",
    "_ = plt.hist(len_eng, bins = 30)\n",
    "plt.title('Max sentence length (English): ' + str(max(len_eng)))\n",
    "plt.subplot(1,2,2)\n",
    "_ = plt.hist(len_germ, bins = 30)\n",
    "plt.title('Max sentence length (German): ' + str(max(len_germ)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and Padding :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    tokenizer = Tokenizer(lower = False, split=' ')\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    \n",
    "    tokenized_data = tokenizer.texts_to_sequences(data)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    return tokenized_data, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary size: 6135\n",
      "German Vocabulary size: 10105\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "eng_data, eng_word_index = tokenize(eng_germ['english'])\n",
    "germ_data, germ_word_index = tokenize(eng_germ['german'])\n",
    "\n",
    "eng_word_index = {i: word for word, i in eng_word_index.items()}\n",
    "germ_word_index = {i: word for word, i in germ_word_index.items()}\n",
    "\n",
    "# (+1) is for padding - '0'\n",
    "eng_vocab_size = len(eng_word_index) + 1\n",
    "germ_vocab_size = len(germ_word_index) + 1\n",
    "\n",
    "print('English Vocabulary size:',eng_vocab_size)\n",
    "print('German Vocabulary size:',germ_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "eng_max_len = max(len_eng)\n",
    "germ_max_len = max(len_germ)\n",
    "\n",
    "# We use post pading because the model needs to know where (timestamp = 1) the sentece starts\n",
    "x = pad_sequences(germ_data, maxlen=germ_max_len, padding='post')\n",
    "y = pad_sequences(eng_data, maxlen=eng_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which converts numbers back to words\n",
    "def convert_int2word(data):\n",
    "    \n",
    "    text = []\n",
    "    for line in data:\n",
    "        sentence = ' '.join([eng_word_index.get(i) for i in line if eng_word_index.get(i) is not None])\n",
    "        \n",
    "        sentence = ' '.join([i for i in sentence.split() if i != 'BOS'])\n",
    "        sentence = ' '.join([i for i in sentence.split() if i != 'EOS'])\n",
    "        \n",
    "        text.append(sentence)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .................................................................................................................................\n",
    "# 4. Model with Attention:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Training Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Dense, LSTM, Embedding, Attention, AdditiveAttention, Concatenate, TimeDistributed, Dot, Activation\n",
    "from tensorflow.keras import Model, Input\n",
    "\n",
    "input_vocab_size = germ_vocab_size\n",
    "output_vocab_size = eng_vocab_size\n",
    "input_maxwords = germ_max_len\n",
    "output_maxwords = eng_max_len - 1\n",
    "hidden_units = 512\n",
    "\n",
    "embedding_size = 300\n",
    "    \n",
    "# Encoder input Embedding...\n",
    "encoder_input = Input(shape = (None,), name = 'encoder_input') \n",
    "encoder_embed_layer = Embedding(input_vocab_size, embedding_size, input_length=input_maxwords, mask_zero=True)\n",
    "encoder_embed_input = encoder_embed_layer(encoder_input)\n",
    "\n",
    "# Encoder part... \n",
    "encoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, name='encoder_lstm')\n",
    "encoder_output, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embed_input)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "    \n",
    "# Decoder input Embedding - Teacher Forcing inputs instead of previous timestep's inputs...\n",
    "decoder_input = Input(shape = (None,), name = 'decoder_input') \n",
    "decoder_embed_layer = Embedding(output_vocab_size, embedding_size, input_length=output_maxwords, mask_zero=True)\n",
    "decoder_embed_input = decoder_embed_layer(decoder_input)\n",
    "\n",
    "# Decoder part... \n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_output, _ , _ = decoder_lstm(decoder_embed_input, initial_state=encoder_state)\n",
    "    \n",
    "\n",
    "# Attention Layer...Luong's Attention Mechanism...\n",
    "attention_value_e = Dot(axes=[2, 2], name='attention_score_e')([decoder_output, encoder_output])  # using 'Dot product' -----> (1 of 3 available methods mentioned in Luong's paper)\n",
    "attention_weights = Activation('softmax', name='attention_vector')(attention_value_e)\n",
    "context_vector = Dot(axes=[2,1], name='context_vector')([attention_weights, encoder_output])    \n",
    "    \n",
    "# attention_layer = AdditiveAttention(name='attention_layer')\n",
    "# context_vector = attention_layer([decoder_output, encoder_output])\n",
    "\n",
    "concat_layer = Concatenate(axis=-1)\n",
    "decoder_combined_context = concat_layer([context_vector, decoder_output])\n",
    "\n",
    "\n",
    "time_dist_layer = TimeDistributed(Dense(hidden_units, activation=\"tanh\"))\n",
    "output = time_dist_layer(decoder_combined_context)\n",
    "    \n",
    "dense_layer = Dense(output_vocab_size, activation='softmax', name='output_dense_layer')\n",
    "decoder_pred = dense_layer(output)\n",
    "\n",
    "# Initializing Model...\n",
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, None, 300)    3031500     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, None, 300)    1840500     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, None, 512),  1665024     embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 512),  1665024     embedding_13[0][0]               \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_score_e (Dot)         (None, None, None)   0           decoder_lstm[0][0]               \n",
      "                                                                 encoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_vector (Activation)   (None, None, None)   0           attention_score_e[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "context_vector (Dot)            (None, None, 512)    0           attention_vector[0][0]           \n",
      "                                                                 encoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 1024)   0           context_vector[0][0]             \n",
      "                                                                 decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 512)    524800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_dense_layer (Dense)      (None, None, 6135)   3147255     time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 11,874,103\n",
      "Trainable params: 11,874,103\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 23450 samples, validate on 10050 samples\n",
      "Epoch 1/40\n",
      "23450/23450 [==============================] - 216s 9ms/sample - loss: 3.3146 - sparse_categorical_accuracy: 0.2108 - val_loss: 2.5789 - val_sparse_categorical_accuracy: 0.2885\n",
      "Epoch 2/40\n",
      "23450/23450 [==============================] - 211s 9ms/sample - loss: 2.4253 - sparse_categorical_accuracy: 0.3506 - val_loss: 2.3179 - val_sparse_categorical_accuracy: 0.3950\n",
      "Epoch 3/40\n",
      "23450/23450 [==============================] - 212s 9ms/sample - loss: 2.2381 - sparse_categorical_accuracy: 0.4107 - val_loss: 2.1988 - val_sparse_categorical_accuracy: 0.4318\n",
      "Epoch 4/40\n",
      "23450/23450 [==============================] - 210s 9ms/sample - loss: 2.1061 - sparse_categorical_accuracy: 0.4428 - val_loss: 2.0906 - val_sparse_categorical_accuracy: 0.4573\n",
      "Epoch 5/40\n",
      "23450/23450 [==============================] - 212s 9ms/sample - loss: 1.9885 - sparse_categorical_accuracy: 0.4695 - val_loss: 1.9875 - val_sparse_categorical_accuracy: 0.4787\n",
      "Epoch 6/40\n",
      "23450/23450 [==============================] - 612s 26ms/sample - loss: 1.8773 - sparse_categorical_accuracy: 0.4900 - val_loss: 1.8989 - val_sparse_categorical_accuracy: 0.4991\n",
      "Epoch 7/40\n",
      "23450/23450 [==============================] - 231s 10ms/sample - loss: 1.7669 - sparse_categorical_accuracy: 0.5127 - val_loss: 1.8088 - val_sparse_categorical_accuracy: 0.5174\n",
      "Epoch 8/40\n",
      "23450/23450 [==============================] - 235s 10ms/sample - loss: 1.6528 - sparse_categorical_accuracy: 0.5346 - val_loss: 1.7136 - val_sparse_categorical_accuracy: 0.5388\n",
      "Epoch 9/40\n",
      "23450/23450 [==============================] - 207s 9ms/sample - loss: 1.5419 - sparse_categorical_accuracy: 0.5570 - val_loss: 1.6250 - val_sparse_categorical_accuracy: 0.5586\n",
      "Epoch 10/40\n",
      "23450/23450 [==============================] - 172s 7ms/sample - loss: 1.4303 - sparse_categorical_accuracy: 0.5815 - val_loss: 1.5440 - val_sparse_categorical_accuracy: 0.5792\n",
      "Epoch 11/40\n",
      "23450/23450 [==============================] - 172s 7ms/sample - loss: 1.3261 - sparse_categorical_accuracy: 0.6040 - val_loss: 1.4729 - val_sparse_categorical_accuracy: 0.5948\n",
      "Epoch 12/40\n",
      "23450/23450 [==============================] - 184s 8ms/sample - loss: 1.2307 - sparse_categorical_accuracy: 0.6254 - val_loss: 1.4058 - val_sparse_categorical_accuracy: 0.6113\n",
      "Epoch 13/40\n",
      "23450/23450 [==============================] - 178s 8ms/sample - loss: 1.1398 - sparse_categorical_accuracy: 0.6448 - val_loss: 1.3507 - val_sparse_categorical_accuracy: 0.6214\n",
      "Epoch 14/40\n",
      "23450/23450 [==============================] - 177s 8ms/sample - loss: 1.0572 - sparse_categorical_accuracy: 0.6638 - val_loss: 1.2995 - val_sparse_categorical_accuracy: 0.6354\n",
      "Epoch 15/40\n",
      "23450/23450 [==============================] - 177s 8ms/sample - loss: 0.9763 - sparse_categorical_accuracy: 0.6812 - val_loss: 1.2456 - val_sparse_categorical_accuracy: 0.6472\n",
      "Epoch 16/40\n",
      "23450/23450 [==============================] - 177s 8ms/sample - loss: 0.8976 - sparse_categorical_accuracy: 0.7005 - val_loss: 1.2011 - val_sparse_categorical_accuracy: 0.6588\n",
      "Epoch 17/40\n",
      "23450/23450 [==============================] - 175s 7ms/sample - loss: 0.8215 - sparse_categorical_accuracy: 0.7199 - val_loss: 1.1583 - val_sparse_categorical_accuracy: 0.6690\n",
      "Epoch 18/40\n",
      "23450/23450 [==============================] - 174s 7ms/sample - loss: 0.7493 - sparse_categorical_accuracy: 0.7398 - val_loss: 1.1193 - val_sparse_categorical_accuracy: 0.6814\n",
      "Epoch 19/40\n",
      "23450/23450 [==============================] - 174s 7ms/sample - loss: 0.6774 - sparse_categorical_accuracy: 0.7602 - val_loss: 1.0775 - val_sparse_categorical_accuracy: 0.6924\n",
      "Epoch 20/40\n",
      "23450/23450 [==============================] - 175s 7ms/sample - loss: 0.6090 - sparse_categorical_accuracy: 0.7821 - val_loss: 1.0471 - val_sparse_categorical_accuracy: 0.7027\n",
      "Epoch 21/40\n",
      "23450/23450 [==============================] - 174s 7ms/sample - loss: 0.5466 - sparse_categorical_accuracy: 0.8028 - val_loss: 1.0191 - val_sparse_categorical_accuracy: 0.7112\n",
      "Epoch 22/40\n",
      "23450/23450 [==============================] - 173s 7ms/sample - loss: 0.4878 - sparse_categorical_accuracy: 0.8240 - val_loss: 0.9978 - val_sparse_categorical_accuracy: 0.7180\n",
      "Epoch 23/40\n",
      "23450/23450 [==============================] - 704s 30ms/sample - loss: 0.4349 - sparse_categorical_accuracy: 0.8434 - val_loss: 0.9741 - val_sparse_categorical_accuracy: 0.7285\n",
      "Epoch 24/40\n",
      "23450/23450 [==============================] - 203s 9ms/sample - loss: 0.3846 - sparse_categorical_accuracy: 0.8633 - val_loss: 0.9534 - val_sparse_categorical_accuracy: 0.7358\n",
      "Epoch 25/40\n",
      "23450/23450 [==============================] - 212s 9ms/sample - loss: 0.3407 - sparse_categorical_accuracy: 0.8800 - val_loss: 0.9424 - val_sparse_categorical_accuracy: 0.7396\n",
      "Epoch 26/40\n",
      "23450/23450 [==============================] - 210s 9ms/sample - loss: 0.3018 - sparse_categorical_accuracy: 0.8952 - val_loss: 0.9256 - val_sparse_categorical_accuracy: 0.7456\n",
      "Epoch 27/40\n",
      "23450/23450 [==============================] - 214s 9ms/sample - loss: 0.2686 - sparse_categorical_accuracy: 0.9066 - val_loss: 0.9237 - val_sparse_categorical_accuracy: 0.7474\n",
      "Epoch 28/40\n",
      "23450/23450 [==============================] - 211s 9ms/sample - loss: 0.2390 - sparse_categorical_accuracy: 0.9174 - val_loss: 0.9143 - val_sparse_categorical_accuracy: 0.7516\n",
      "Epoch 29/40\n",
      "23450/23450 [==============================] - 213s 9ms/sample - loss: 0.2124 - sparse_categorical_accuracy: 0.9269 - val_loss: 0.9148 - val_sparse_categorical_accuracy: 0.7513\n",
      "Epoch 30/40\n",
      "23450/23450 [==============================] - 212s 9ms/sample - loss: 0.1888 - sparse_categorical_accuracy: 0.9357 - val_loss: 0.9083 - val_sparse_categorical_accuracy: 0.7554\n",
      "Epoch 31/40\n",
      "23450/23450 [==============================] - 216s 9ms/sample - loss: 0.1692 - sparse_categorical_accuracy: 0.9423 - val_loss: 0.9056 - val_sparse_categorical_accuracy: 0.7577\n",
      "Epoch 32/40\n",
      "23450/23450 [==============================] - 229s 10ms/sample - loss: 0.1505 - sparse_categorical_accuracy: 0.9492 - val_loss: 0.9054 - val_sparse_categorical_accuracy: 0.7596\n",
      "Epoch 33/40\n",
      "23450/23450 [==============================] - 180s 8ms/sample - loss: 0.1367 - sparse_categorical_accuracy: 0.9535 - val_loss: 0.9040 - val_sparse_categorical_accuracy: 0.7607\n",
      "Epoch 34/40\n",
      "23450/23450 [==============================] - 179s 8ms/sample - loss: 0.1215 - sparse_categorical_accuracy: 0.9593 - val_loss: 0.9052 - val_sparse_categorical_accuracy: 0.7610\n",
      "Epoch 35/40\n",
      "23450/23450 [==============================] - 180s 8ms/sample - loss: 0.1094 - sparse_categorical_accuracy: 0.9637 - val_loss: 0.9082 - val_sparse_categorical_accuracy: 0.7623\n",
      "Epoch 36/40\n",
      "23450/23450 [==============================] - 181s 8ms/sample - loss: 0.0984 - sparse_categorical_accuracy: 0.9674 - val_loss: 0.9111 - val_sparse_categorical_accuracy: 0.7630\n",
      "Epoch 37/40\n",
      "23450/23450 [==============================] - 180s 8ms/sample - loss: 0.0907 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.9167 - val_sparse_categorical_accuracy: 0.7641\n",
      "Epoch 38/40\n",
      "23450/23450 [==============================] - 180s 8ms/sample - loss: 0.0856 - sparse_categorical_accuracy: 0.9715 - val_loss: 0.9194 - val_sparse_categorical_accuracy: 0.7624\n",
      "Epoch 39/40\n",
      "23450/23450 [==============================] - 180s 8ms/sample - loss: 0.0773 - sparse_categorical_accuracy: 0.9739 - val_loss: 0.9174 - val_sparse_categorical_accuracy: 0.7648\n",
      "Epoch 40/40\n",
      "23450/23450 [==============================] - 181s 8ms/sample - loss: 0.0709 - sparse_categorical_accuracy: 0.9760 - val_loss: 0.9244 - val_sparse_categorical_accuracy: 0.7633\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "epochs = 40\n",
    "batch_size=512\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n",
    "\n",
    "# Assigning appropriate inputs and outputs\n",
    "training_encoder_input = x_train\n",
    "training_decoder_input = y_train[:,:-1]\n",
    "training_decoder_output = y_train[:,1:]\n",
    "\n",
    "filepath = './model weigths/AttentionModel_weights_40epochs.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(x=[training_encoder_input, training_decoder_input], \n",
    "                    y=[training_decoder_output],\n",
    "                    epochs=epochs, batch_size=batch_size, validation_split = 0.3,\n",
    "                    callbacks=[checkpoint],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Test Model: (for prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder during test time...\n",
    "encoder_model = Model(inputs = encoder_input, outputs = [encoder_output, encoder_state])\n",
    "\n",
    "# Decoder during test time\n",
    "decoder_embed_input_test = decoder_embed_layer(decoder_input)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(None,)) # Note: shape - doesn't include no of training data\n",
    "decoder_state_input_c = Input(shape=(None,))\n",
    "decoder_input_state = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_output, decoder_state_h, decoder_state_c = decoder_lstm(decoder_embed_input_test, initial_state=decoder_input_state)\n",
    "decoder_output_state = [decoder_state_h, decoder_state_c]\n",
    "\n",
    "decoder_model = Model(inputs = [decoder_input] + decoder_input_state, \n",
    "                     outputs = [decoder_output] + decoder_output_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention during test time\n",
    "\n",
    "att_decoder_output = Input(shape=(None,hidden_units))\n",
    "att_encoder_output = Input(shape=(None,hidden_units))   \n",
    "\n",
    "attention_value_e = Dot(axes=[2, 2], name='attention_score_e')([att_decoder_output, att_encoder_output])\n",
    "attention_weights = Activation('softmax', name='attention_vector')(attention_value_e)\n",
    "context_vector = Dot(axes=[2,1], name='context_vector')([attention_weights, att_encoder_output])    \n",
    "\n",
    "# context_vector = attention_layer([att_decoder_output, att_encoder_output])\n",
    "\n",
    "decoder_combined_context = concat_layer([context_vector, att_decoder_output])\n",
    "\n",
    "output = time_dist_layer(decoder_combined_context)    \n",
    "\n",
    "decoder_prediction = dense_layer(output)\n",
    "\n",
    "attention_model = Model(inputs = [att_decoder_output, att_encoder_output],\n",
    "                       outputs = [decoder_prediction, attention_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Decode the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode sequence function here....\n",
    "def decode_sequence(input_seq):\n",
    "    \n",
    "    input_len = len(input_seq)\n",
    "    \n",
    "    encoder_output, encoder_state_h, encoder_state_c = encoder_model.predict(input_seq)\n",
    "    encoder_states = [encoder_state_h, encoder_state_c]\n",
    "    \n",
    "    target_seq = np.ones((input_len,1)) # index corresponding to BOS\n",
    "\n",
    "    decoded_sentence = [''] * input_len\n",
    "    attention_wts = []\n",
    "    max_len = eng_max_len - 1\n",
    "    attention_wts = np.array([])\n",
    "    \n",
    "    count = max_len\n",
    "    while count > 0:\n",
    "        \n",
    "        decoder_output, h, c = decoder_model.predict([target_seq] + encoder_states)\n",
    "                \n",
    "        output, attention_weights = attention_model.predict([decoder_output, encoder_output])        \n",
    "        attention_wts = np.hstack([attention_wts, attention_weights]) if attention_wts.size else attention_weights\n",
    "        sampled_token_index = np.argmax(output, axis = 2)\n",
    "\n",
    "        for i in range(input_len):\n",
    "            predicted_word = eng_word_index.get(sampled_token_index[i][0])\n",
    "            if predicted_word is not None:\n",
    "                decoded_sentence[i] += predicted_word + ' '\n",
    "                \n",
    "        target_seq = sampled_token_index\n",
    "        states_value = [h,c]\n",
    "        \n",
    "        count = count - 1\n",
    "        \n",
    "    return decoded_sentence, attention_wts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Test prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_input = x_test[:10]\n",
    "test_decoder_input = y_test[:,:-1][:10]\n",
    "test_decoder_output = y_test[:,1:][:10]\n",
    "\n",
    "decoded_sentence,attention_weights = decode_sequence(test_encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 18)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no nobody nobody nobody nobody nobody nobody nobody nobody nobody ',\n",
       " 'tom was tom was tom was tom was tom was ',\n",
       " 'i am i am i am i am i am ',\n",
       " 'you you you you you you you you you you ',\n",
       " 'is is is is is is is is is is ',\n",
       " 'who who who who who who who who who who ',\n",
       " 'i can i can i can i can i can ',\n",
       " 'what what what what what what what what what what ',\n",
       " 'that is that is that is that is that is ',\n",
       " 'did did did did did did did did did did ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German: wer hat die apfel gestohlen <NONE> <NONE> <NONE> <NONE> <NONE> <NONE> <NONE> <NONE> <NONE> <NONE> <NONE> <NONE> <NONE>\n",
      "Actual: who stole the apples\n",
      "Predicted: who who who who who who who who who who \n"
     ]
    }
   ],
   "source": [
    "def germ_convert_int2word(data):\n",
    "        text = []\n",
    "        for line in data:\n",
    "            sentence = ' '.join([germ_word_index.get(i) if germ_word_index.get(i) is not None else '<NONE>' for i in line])\n",
    "            # sentence = ' '.join([i for i in sentence.split() if i != 'EOS'])\n",
    "            text.append(sentence)\n",
    "        return text\n",
    "    \n",
    "index = 5\n",
    "print('German:',germ_convert_int2word(test_encoder_input)[index])\n",
    "print('Actual:',convert_int2word(test_decoder_output)[index])\n",
    "print('Predicted:',decoded_sentence[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 1.1665504e-33, 2.7060440e-21, 7.5876080e-14,\n",
       "       7.1428575e-02, 7.1428575e-02, 7.1428575e-02, 7.1428575e-02,\n",
       "       7.1428575e-02, 7.1428575e-02, 7.1428575e-02, 7.1428575e-02,\n",
       "       7.1428575e-02, 7.1428575e-02, 7.1428575e-02, 7.1428575e-02,\n",
       "       7.1428575e-02, 7.1428575e-02], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[index][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ca18003488>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap((attention_weights[index]), linewidths = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get away',\n",
       " 'tom drives a truck',\n",
       " 'can i pay later',\n",
       " 'money cant buy life',\n",
       " 'tom wont be missed',\n",
       " 'tom is a very shy boy',\n",
       " 'i have to feed my cat',\n",
       " 'tom is very busy now',\n",
       " 'they were shocked',\n",
       " 'they said its ok']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_int2word(test_decoder_output)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.524960\n",
      "BLEU-2: 0.000000\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "actual = convert_int2word(y_test)\n",
    "# actual = [line.split() for line in actual]\n",
    "predicted = convert_int2word(np.argmax(preds, axis = 2))\n",
    "# predicted = [line.split() for line in predicted]\n",
    "\n",
    "# calculate BLEU score\n",
    "print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
