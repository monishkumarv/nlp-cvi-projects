{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers: Attention is All You Need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In transformers no RNN is used for decoder. The input for each decoder is just the context vector (using attention). The hidden state of previous timestep is not used as an input (which is the reason why parrallel computation is possible and as a result Transformers work faster - check????).\n",
    "- Positional Encoding - Since we are not using any RNNs we wont be able to capture the order/position of words in a sentence. In order to capture that we use positional encoding. \"\"\"\"Done using a sin wave\"\"\"\"\n",
    "- Attention weights - dot product of Query and Key (semma intiution)\n",
    "- Encoder of the input sentence gives the 'Key,Value' pairs. Encoder of the target sentence gives the 'Queries'.\n",
    "- Masking - the Transformer applies a mask to the input in the first multi-head attention module to avoid seeing potential ‚Äòfuture‚Äô sequence elements. This is specific to the Transformer architecture because we do not have RNNs where we can input our sequence sequentially. Here, we input everything together and if there were no mask, the multi-head attention would consider the whole decoder input sequence at each position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to translate a French sentence without having the German sentence:\n",
    "The trick here is to re-feed our model for each position of the output sequence until we come across an end-of-sentence token.\n",
    "A more step by step method would be:\n",
    "Input the full encoder sequence (French sentence) and as decoder input, we take an empty sequence with only a start-of-sentence token on the first position. This will output a sequence where we will only take the first element.\n",
    "That element will be filled into second position of our decoder input sequence, which now has a start-of-sentence token and a first word/character in it.\n",
    "Input both the encoder sequence and the new decoder sequence into the model. Take the second element of the output and put it into the decoder input sequence.\n",
    "Repeat this until you predict an end-of-sentence token, which marks the end of the translation.\n",
    "We see that we need multiple runs through our model to translate our sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple NN - no context, no postion\n",
    "- Simple LSTM encoder-decoder - captures context, no position (check!! - each timestep represents a position)\n",
    "- Transformers - captures both context and position of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers - Recurrent Networks were, until now, one of the best ways to capture the timely dependencies in sequences. However, the team presenting the paper proved that an architecture with only attention-mechanisms without any RNN (Recurrent Neural Networks) can improve on the results in translation task and other tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one fo LSTM's limitation is that Transfer Learning never really worked.Unlike ConvNets, you cant train on a different dataset and use it for your problem statement. You have to train on a specific dataset every time.\n",
    "\n",
    "In lstms (takes O(n^2) time) we can predict the next word only after predicting the previous word - so parallel computation is impossible (using gpu: 'Am i a joke to you?') - But with transformers you can do parallel computation - Even though the time required is O(n^2) for transformers, this time can be saved by using a gpu coz the input words can be passed in parallel unlike LSTM (input words are passed one by one).\n",
    " \n",
    " with transformers - transfer learning is possible\n",
    " \n",
    " Multi-headed attention: Doing the attention process 8 times - so that you can pay induvidual attention to 8 features of a sentence such as one for vocabulary, one for grammer, one for tense, etc...\n",
    " \n",
    " Positional encoding: gives a vector with 'context based on position' -  Since we have no recurrent networks that can remember how sequences are fed into a model, we need to somehow give every word/part in our sequence a relative position since a sequence depends on the order of its elements. These positions are added to the embedded representation (n-dimensional vector) of each word.\n",
    " \n",
    " Self attention?\n",
    " \n",
    " When LSTM over transformers?: when sentence length is long or infinite coz transformers take N^2 time.\n",
    " \n",
    " Implementation code - hugging face - pre-trained transformer library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The type of attention that uses all the encoder hidden states is also known as global attention. In contrast, local attention uses only a subset of the encoder hidden states. As the scope of this article is global attention, any references made to ‚Äúattention‚Äù in this article are taken to mean ‚Äúglobal attention.‚Äù\n",
    " \n",
    " This article provide a summary of how attention works using animations, so that we can understand them without (or after having read a paper or tutorial full of) mathematical notations üò¨\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
