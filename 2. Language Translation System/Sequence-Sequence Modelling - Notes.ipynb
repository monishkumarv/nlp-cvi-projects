{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam (great for starting) is fast, but tends to overfit. SGD is slow but gives great results (squeezes the juice of the model). Somtimes RMS also works best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot vs dot: keras layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (99, 600, 300)\n"
     ]
    }
   ],
   "source": [
    "# Imagine there are 99 of matrix of shape 5000x600 and 99 matrix of shape 5000x300\n",
    "\n",
    "from keras.layers import Input, dot\n",
    "input_a = Input(batch_shape=(99,5000,600))\n",
    "input_b = Input(batch_shape=(99,5000,300))\n",
    "\n",
    "element_wise_dot_product = dot([input_a,input_b], axes = [1,1])\n",
    "print('Shape:',element_wise_dot_product.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (99, 600, 300)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, dot\n",
    "input_a = Input(batch_shape=(99,600,5000))\n",
    "input_b = Input(batch_shape=(99,5000,300))\n",
    "\n",
    "element_wise_dot_product = dot([input_a,input_b], axes = [2,1])\n",
    "print('Shape:',element_wise_dot_product.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Useful References:\n",
    "\n",
    "- Andrew Ng notes: Analytics Vidhya - [link](https://www.analyticsvidhya.com/blog/2019/01/sequence-models-deeplearning/)\n",
    "- <b>Stanford Video lecture</b> - Neural Machine Translation and Models with Attention - [link](https://youtu.be/IxQtK2SjWWM)\n",
    "- What is Teacher Forcing - [link](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/)\n",
    "- Basic Seq2Seq modelling in keras (with attention) with code: Analytics Vidhya - [link](https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/)\n",
    "- Semma perfectly organised blog - <b>nearly all your doubts got cleared</b> - [link](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)\n",
    "- Attention Mechanism - [link](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/)\n",
    "- Attention official paper: <b>Luong et al., 2015’s Attention Mechanism</b> - [link](https://www.aclweb.org/anthology/D15-1166.pdf) - very easy to read and understand (Section 3)\n",
    "- Attention official paper: <b>Bahdanau’s Attention Mechanism</b> - [link](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "- BRNN with attention (video) - Bahdanau's method - [link](https://youtu.be/6l1fv0dSIg4)\n",
    "- Code (with attention) - [link](https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html)\n",
    "- Code for Test Prediction without Teacher Forcing Method - [link](https://medium.com/analytics-vidhya/machine-translation-encoder-decoder-model-7e4867377161)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seq-Seq model architecture: Encoder-Decoder model\n",
    "- For the encoder, we will use an embedding layer and an LSTM layer - (Input: German Sentence, Output: Context)\n",
    "- For the decoder, we will use another LSTM layer followed by a dense layer - (Input: [Context, previous timestamp's output], Output: English Sentence)\n",
    "\n",
    "<img style=\"float: left;\" src=\"./assets/encoder_decoder2.jpg\">\n",
    "<img style=\"float: center;\" src=\"./assets/gru_vs_lstm1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"./assets/lstm_cell.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical terms used in Seq2seq models with attention:\n",
    "- <b>Key:</b> hidden state of the encoder - encoder_output\n",
    "- <b>Query:</b> hidden state of the decoder from the previous timestep - \n",
    "- <b>Value:</b> attention weights (i guess) or input token??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Difference Between Return Sequences and Return States for LSTMs in Keras:\n",
    "- <b>return_sequences:</b> If set as False, then it outputs the hidden state value (ht) of the last cell alone. If set as True, it outputs ht value of all cells. If we add a dense layer, it gets added on the top each 'ht'.\n",
    "- <b>return_state:</b> If set as True, it returns lstm_cell_output (ht), state_h (same ht again!), state_c (ct i.e. the cell state) of the the last cell alone!!\n",
    "- <b> If both return_sequence and return_state is set as True,</b> then the layer returns the hidden state(ht) for each cell (timestep), then separately, the hidden state output for the last timestep (last cell) and the cell state (ct) for the last  timestep (last cell).\n",
    "\n",
    "Reference link nice examples - https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other important Attributes and Methods: Keras\n",
    "- <b>mask_zero:</b> Boolean, whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input.\n",
    "\n",
    "- <b>RepeatVector:</b> We know that the input at each timestep in the Decoder part of the network is the output (y) of the dense softmax layer of the previous timestep (Teacher Forcing method). Instead of that we can also use the hidden state of the last timestep of the encoder lstm as input to each cell of the decoder lstm. So repeat vector is used to create n copies (n = no of decoder timestep = max length of output) of the hidden state vector optained from the last cell of the encoder part.\n",
    "\n",
    "- <b>RepeatVector:</b> Refer this [link](https://www.wandb.com/classes/intro/class-10-notes) for pictorial representation.\n",
    "\n",
    "- <b>return_sequences:</b> LSTM will eat the words of your sentence one by one, you can chose via \"return_sequence\" to outuput something (the state) at each step (after each word processed) or only output something after the last word has been eaten. So with return_sequence=TRUE, the output will be a sequence of the same length, with return_sequence=FALSE, the output will be just one vector. <b>Note:</b> The dense layer will be applied to all the cells of the decoder part of the LSTM network. By default the weights of all the layers are identical for each cell.\n",
    "\n",
    "- <b>TimeDistributed:</b> \n",
    "[link](https://stackoverflow.com/questions/55532683/why-is-timedistributed-not-needed-in-my-keras-lstm) & \n",
    "[link](https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters/44616780#44616780)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse_categorical_crossentropy vs categorical_crossentropy: Loss functions in keras\n",
    "- If you use 'categorical_crossentropy' you use one hot encoding, and if you use 'sparse_categorical_crossentropy' you encode as normal integers (index of the particular one-hot value, i.e. the position of the '1' in the one-hot encoded vector). In our case we can use either (don't forget to modify the output appropriately)\n",
    "- But if this is a multi-label classification (i.e. one input may belong to more than one class), then you can only use categorical_crossentropy\n",
    "\n",
    "- Reference: [link](https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints: Keras\n",
    "- Loading and saving a model in keras - [link](https://machinelearningmastery.com/check-point-deep-learning-models-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Mechanism : [semma link - nearly every doubts you had got cleared](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)\n",
    "The context vector has been given the responsibility of encoding all the information in a given source sentence in to a vector of few hundred elements. Now to give a bit of context, this vector needs to preserve:\n",
    "- Information about subject, object and verb\n",
    "- Interactions between these entities\n",
    "\n",
    "This can be quite daunting especially for long sentences. \n",
    "Therefore a better solution was needed to push the boundaries.What if instead of relying just on the context vector, the decoder had access to all the past states of the encoder? That’s exactly what <b>Attention</b> is doing. At each decoding step, the decoder gets to look at any particular state of the encoder. \n",
    "\n",
    "<b>Important Note:</b> For long sentences (20-30 words) LSTMs can do a decent job of holding long dependencies (context), but for even longer sentences (50-60 words) LSTMs are of not much use. In those cases introducing Attention Mechanism to the models we get <b>drastic improvements in accuracy (BLEU score)</b> when compared to models with no attention. It doesn't mean that for shorter sentences there is no need to use attention mechanism. Even if we use attention mechanism for short sentences (15-20 words), we can see significant improvement in the results. \n",
    "\n",
    "<i>Weird results: The score drops down for sentences with less than 5 words - <b>Refer Stanford video lecture for more info - [link](https://youtu.be/IxQtK2SjWWM)</b></i>\n",
    "\n",
    "<b>There are two main Attention techniques.</b> They are,\n",
    "- Bahdanau's Attention\n",
    "- Luong's Attention\n",
    "Here we will be using <b>Luong et al., 2015’s Attention Mechanism</b> . This is not the one mentioned by Andrew Ng (that is . <b>Bahdanau's Attention</b> - i guess). \n",
    "\n",
    "<b>Types of Attentions:</b> Global and Local Attention\n",
    "\n",
    "<b>Attention Model Architecture explained -</b> https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizations for seq2seq modelling:\n",
    "\n",
    "- <b>Attention:</b> The input to the decoder is a single vector which has to store all the information about the context. This becomes a problem with large sequences. Hence the attention mechanism is applied which allows the decoder to look at the input sequence selectively.\n",
    "- <b>Beam Search:</b> The highest probability word is selected as the output by the decoder. But this does not always yield the best results, because of the basic problem of greedy algorithms. Hence beam search is applied which suggests possible translations at each step. This is done making a tree of top k-results.\n",
    "- <b>Bucketing:</b> Variable-length sequences are possible in a seq2seq model because of the padding of 0’s which is done to both input and output. However, if the max length set by us is 100 and the sentence is just 3 words long it causes huge wastage of space. So we use the concept of bucketing. We make buckets of different sizes like (4, 8) (8, 15) and so on, where 4 is the max input length defined by us and 8 is the max output length defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation metric: BLEU (Bilingual Evaluation Understudy) score\n",
    "- <b>Layman defeniton:</b> In some cases there might be two equally good translations for a particular sentence. If the predicted translation is same as either of the 2 translations the BLEU score is given as '1'. If the predicted doesn't match with neither of the '2 actual translations' then the BLEU score will be '0'.\n",
    "- For a detailed explanation refer [link](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/#:~:text=Crash%2DCourse%20Now-,Bilingual%20Evaluation%20Understudy%20Score,in%20a%20score%20of%200.0.) or <b>Andrew Ng's</b> video - C5W3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ideas to try out to get an improved Accuracy!\n",
    "This section lists some ideas that you may wish to explore.\n",
    "\n",
    "- <b>Data Cleaning: </b> Different data cleaning operations could be performed on the data, such as not removing punctuation or normalizing case, or perhaps removing duplicate English phrases.\n",
    "- <b>Vocabulary: </b> The vocabulary could be refined, perhaps removing words used less than 5 or 10 times in the dataset and replaced with “unk“.\n",
    "- <b>More Data: </b> The dataset used to fit the model could be expanded to 50,000, 100,000 phrases, or more. \n",
    "- <b>Input Order: </b> The order of input phrases could be reversed (instead of passing the original order of words), which has been reported to lift skill (or) a Bidirectional input layer could be used.\n",
    "- <b>Layers: </b> The encoder and/or the decoder models could be expanded with additional layers and trained for more epochs, providing more representational capacity for the model.\n",
    "- <b>Units: </b> The number of memory units in the encoder and decoder could be increased, providing more representational capacity for the model.\n",
    "- <b>Regularization: </b> The model could use regularization, such as weight or activation regularization, or the use of dropout on the LSTM layers.\n",
    "- <b>Pre-Trained Word Vectors: </b> Pre-trained word vectors could be used in the model.\n",
    "- <b>Recursive Model: </b> A recursive formulation of the model could be used where the next word in the output sequence could be conditional on the input sequence and the output sequence generated so far (this is a method other than Teacher Forcing).\n",
    "- <b>Hard Attention</b> and <b>Hierarchial Attention</b> will be of help for longer sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter tuning, Attention, etc.:\n",
    "This link which explains on how you should go about when doing Hyperparameter Tuning for <b>Encoder-Decoder type models</b>:\n",
    "\n",
    "[How to Configure an Encoder-Decoder Model for NMT](https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/)\n",
    "\n",
    "<img style=\"float: left;\" src=\"./assets/best_hyperparameters_nmt1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Keras Modelling:\n",
    "\n",
    "<b>3 ways to create Keras model:</b>\n",
    "- Sequential API - A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "- Functional API - The Keras functional API is a way to create models that is more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, models with shared layers, and models with multiple inputs or outputs.The main idea that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.\n",
    "- Model subclassing - \n",
    "\n",
    "##### model.save() - This saved file includes the:\n",
    "\n",
    "- model architecture\n",
    "- model weight values (that were learned during training)\n",
    "- model training config, if any (as passed to compile)\n",
    "- optimizer and its state, if any (to restart training where you left off)\n",
    "\n",
    "##### TensorFlow or Keras? Which one should I learn? - [link](https://medium.com/implodinggradients/tensorflow-or-keras-which-one-should-i-learn-5dd7fa3f9ca0)\n",
    "If you are not doing some research purpose work or developing some special kind of neural network, then go for Keras (trust me, I am a Keras fan!!). And it’s super easy to quickly build even very complex models in Keras.\n",
    "If you want more control over your network and want to watch closely what happens with the network over the time, TF is the right choice (though the syntax can give you nightmares sometimes). But as we all know that Keras has been integrated in TF, it is wiser to build your network using tf.keras and insert anything you want in the network using pure TensorFlow. \n",
    "\n",
    "In short,<br>\n",
    "tf.keras + tf = All you ever gonna need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
